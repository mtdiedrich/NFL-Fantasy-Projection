{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Saving Data\n",
    "Web scrapers face a unique set of challenges, the most interesting of which may be the adversarial nature of it all. In general, webmasters do not want their data scraped, and they have a handful of tools available to them to inconvenience the webscraper. The site from which we'll be scraping data, profootballreference, deters webscrapers by temporarily blocking traffic from users that make too many requests in too short of a time period. To counteract this, we'll make use of python's `time.sleep()` function, but, where possible, we'll also be saving webpages locally. This will allow us ad hoc access without the need to wait through numerous calls to `time.sleep()`.\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "Naturally, we must begin with collecting the data. This will involve scraping profootballreference.\n",
    "\n",
    "Let's define our imports and some helper functions to keep our code organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import html5lib\n",
    "\n",
    "\n",
    "def get_passing_url(year):\n",
    "    return f\"https://www.pro-football-reference.com/years/{year}/passing.htm\"\n",
    "\n",
    "\n",
    "def get_rushing_url(year):\n",
    "    return f\"https://www.pro-football-reference.com/years/{year}/rushing.htm\"\n",
    "\n",
    "\n",
    "def get_receiving_url(year):\n",
    "    return f\"https://www.pro-football-reference.com/years/{year}/receiving.htm\"\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_valid_subdirectories(soup):\n",
    "    html_subdirectories = soup.find_all(\"a\")\n",
    "    valid_subdirectories = [link for link in html_subdirectories if '/players/' in str(link)]\n",
    "    subdirectories = [link.get('href') for link in valid_subdirectories]\n",
    "    return subdirectories\n",
    "\n",
    "\n",
    "def make_directory(name):\n",
    "    import os\n",
    "    if not os.path.exists(name):\n",
    "        os.makedirs(name)\n",
    "        \n",
    "        \n",
    "def save_html(soup, name):\n",
    "    with open(name, \"w\") as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now our first step will involve iterating through each season of interest and getting the webpages for that season's passing, rushing, and receiving stats. We will be saving those pages locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Note on Seasons\n",
    "\n",
    "How many seasons should would be interested in? That's a difficult question to answer. The naive perspective suggest that we want *all* of the available data, and, while there would be some merit to that approach, anyone familiar wih the NFL knows that the game has evolved over time; the way that the game was played in its earliest form has little in common with the game as it exists today, and historical data would feature trends that are not mirrored today. For that reason, we want to focus on \"modern\" data.\n",
    "\n",
    "Still, defining 'modern\" is not easy. When discussing the NFL, \"modern\" generally refers to any season after the NFL-AFL merger in 1970. In most contexts, this is a good defintion, but, for the purpose of this project, I don't feel that it is sufficient. The way that the game is played and the ways in which players are utilized continued to evolve after the merger, and the data reflects that. For that reason, I had to settle on a different defintion of \"modern.\"\n",
    "\n",
    "For the purposes of this project, the modern NFL began in 2012. In 2011, the union representing the players (the NFL Player's Association) negotiated a new collective bargaining agreement (CBA) with the team owners. The CBA had *massive* ramifications, many of which have surely gone unnoticed, but one of the more visible effects has been the reduced interest in signing veteran players to be minor contributors. To oversimplify a nuanced and fascinating topic: the CBA that took prior to the 2012 season limited earnings for rookie players, increasing their value relative to their veteran counterparts. The demand for low-level veterans plummted because rookie contract players offer 90% of the performance for 50% of the price (numbers that I've fabricated to demonstrate the point). This shift meant that many positions (especially runningback) skewed much younger after the CBA took effect. For this project, we want data that reflects current positional values, thus, we are using only data from after the 2011 CBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_directory(\"data/season/passing\")\n",
    "make_directory(f\"data/season/rushing\")\n",
    "make_directory(f\"data/season/receiving\")\n",
    "\n",
    "for year in tqdm.tqdm(range(2012, 2023)):\n",
    "    # Passing\n",
    "    passing_url = get_passing_url(year)\n",
    "    passing_soup = get_soup(passing_url)\n",
    "    save_html(passing_soup, f\"data/season/passing/{year}.html\")\n",
    "    # Rushing\n",
    "    rushing_url = get_rushing_url(year)\n",
    "    rushing_soup = get_soup(rushing_url)\n",
    "    save_html(rushing_soup, f\"data/season/rushing/{year}.html\")    \n",
    "    # Receiving\n",
    "    receiving_url = get_receiving_url(year)\n",
    "    receiving_soup = get_soup(receiving_url)\n",
    "    save_html(receiving_soup, f\"data/season/receiving/{year}.html\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the season stats pages saved, we now need to parse them in order to extract a list of links to the pages for every player that has recorded a pass, rush, or catch since 2011. From those links, we must save the corresponding webpage to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [56:59<00:00,  5.57s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data_dir = \"data/season\"\n",
    "\n",
    "# Get the list of file paths\n",
    "file_paths = []\n",
    "for dir in os.listdir(data_dir):\n",
    "    if os.path.isdir(os.path.join(data_dir, dir)):\n",
    "        for file in os.listdir(os.path.join(data_dir, dir)):\n",
    "            if file.endswith(\".html\"):\n",
    "                file_paths.append(os.path.join(data_dir, dir, file))\n",
    "                \n",
    "# Convert each file to soup\n",
    "soups = [BeautifulSoup(open(file_path, \"r\").read(), 'html.parser') for file_path in file_paths]\n",
    "\n",
    "# Get the list of player subdirectories\n",
    "player_links = []\n",
    "for soup in soups:\n",
    "    # Passing\n",
    "    passing_table = passing_soup.find(id=\"passing\")\n",
    "    passing_subdirectories = get_valid_subdirectories(passing_table)\n",
    "    player_links.extend(passing_subdirectories)\n",
    "    # Rushing\n",
    "    rushing_table = rushing_soup.find(id=\"rushing\")\n",
    "    rushing_subdirectories = get_valid_subdirectories(rushing_table)\n",
    "    player_links.extend(rushing_subdirectories)\n",
    "    # Receiving\n",
    "    receiving_table = receiving_soup.find(id=\"receiving\")\n",
    "    receiving_subdirectories = get_valid_subdirectories(receiving_table)\n",
    "    player_links.extend(receiving_subdirectories)\n",
    "    \n",
    "# Make unique\n",
    "player_links = list(set(player_links))\n",
    "\n",
    "# Get player soups and save to file\n",
    "make_directory(\"data/players\")\n",
    "for link in tqdm.tqdm(player_links):\n",
    "    url = f\"https://www.pro-football-reference.com{link}\"\n",
    "    soup = get_soup(url)\n",
    "    filename = link.split(\"/\")[-1]\n",
    "    save_html(soup, f\"data/players/{filename}\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have a list of subdirectories for every player that has passed, rushed, or caught a ball since 2011. We can append these subdirectories to a base URL to get links to each player's page.\n",
    "\n",
    "Next we'll have to parse the HTML of each player's page and extract the data. Because doing so would involve several layers of data structure nesting, I opted instead to create a `Player` class to hold each player's data. Because \"Tell, Don't Ask\" programming is a good practice, and because we are good little object-oriented programmers, we'll instantiate the class with just a single subdirectory and have it handle the HTML parsing. Unfortunately, the format of each player's webpage varies depending on the player's position, meaning we need position-specific parsing functionality, and, thus, we will have `Player` be an abstract class, and we'll create position classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "class Player(ABC):\n",
    "    def __init__(self, link):\n",
    "        self.url = f\"https://www.pro-football-reference.com{link}\"\n",
    "        self.soup = self.get_soup()\n",
    "\n",
    "    def get_soup(self):\n",
    "        page = requests.get(self.url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    @abstractmethod\n",
    "    def some_player_method(self):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, link):\n",
    "        position = cls._get_position_static(link)\n",
    "        if position == 'QB':\n",
    "            return Quarterback(link)\n",
    "        else:\n",
    "            return SkillPosition(link)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_position_static(link):\n",
    "        url = f\"https://www.pro-football-reference.com{link}\"\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        position_strong_tag = soup.find('strong', string='Position')\n",
    "        if position_strong_tag and position_strong_tag.next_sibling:\n",
    "            position = position_strong_tag.next_sibling.strip(': ').strip()\n",
    "            return position\n",
    "        return None\n",
    "\n",
    "\n",
    "class Quarterback(Player):\n",
    "    def __init__(self, link):\n",
    "        super().__init__(link)\n",
    "        self._set_throws()\n",
    "        \n",
    "    def some_player_method(self):\n",
    "        print('Quarterback method')\n",
    "\n",
    "    def _set_throws(self):\n",
    "        self.throws = self.get_throws()\n",
    "\n",
    "    def get_throws(self):\n",
    "        throws_strong_tag = self.soup.find('strong', string='Throws:')\n",
    "        if throws_strong_tag and throws_strong_tag.next_sibling:\n",
    "            throws = throws_strong_tag.next_sibling.strip(': ').strip()\n",
    "            return throws\n",
    "        return None\n",
    "    \n",
    "class SkillPosition(Player):\n",
    "    def some_player_method(self):\n",
    "        # Implementation specific to SkillPosition\n",
    "        print('SkillPosition method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Quarterback at 0x2079a2503a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_link = player_links[0]\n",
    "player = Player.create(player_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
